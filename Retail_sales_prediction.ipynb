{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nakulcj7/Retail-sales/blob/main/Retail_sales_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Retail Sales Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "I have been provided with historical sales data for 1,115 Rossmann stores.My task is to forecast the sales for the up comming six weeks in advance and it has been mentioned that some of the store were temporarily closed for rebursment during that period sales were 0. I have two datasets.firstly i imported both the datasets.One dataset is rossman store which contains 1017209 rows and 9 columns.The dataset doesn't contain any null or duplicated values.It gives us inforamation about the type of store,sales in each store by what date through how many number of customers.Also whether the sale was affected by closure of shop on weekends or or any kind of stateholiday or schoolholiday.It also gives on how sales was affected on applying promo.The other dataset is store which has 1115 Rows and 10 Column.This dataset contains too many null values in different columns.It provides information like type of store and what is assostment level used in store,how far away is competitor from store,since how long is competitor there in the market and how often is promo applied on the store.\n",
        "\n",
        "I droped some colunms from store datset which contains too many null values and filled some fearture which contains very less amount of null values and then i merged both the datasets.I then checked for outliers and there were some columns which contained outliers and then treated them using percentile method.\n",
        "\n",
        "Then i performed EDA (univeriate,Biveriate and Multiveriate analysis) to extract some useful insights from the data.Then i performed Feature Enginering and data preprocessing in which i performed label encoding for categirical features and defined some new features also ,i calculated VIF to find out Multicolinearity between the features and selected important feature for futher analysis.Then i splitted the data in dependent and independent variables and transformed the data using Standard Scaler.Then i split the data in x_train,y_train,x_test,y_test with test size as 0.2.\n",
        "\n",
        "Then at the end i implemented Model on my data like Linear Regression,Ridge Regression,Lasso Regression,Elastic Net,Decision Tree,Random Forest and XGBoost model on my data. Based on the performance of all the models, i have selected XGBoost model as the final prediction model. The XGBoost model outperformed the other models in several key aspects, making it the preferred choice for prediction\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Nakulcj7/Retail-sales.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Rossmann operates over 3,000 drug stores in 7 European Countries.Currently,Rossmann store managers are tasked with predicting their daily sales for upto six weeks in advance.Store sales are influenced by many factors,including promotions,competition,school and state holidays,seasonality,and loclity.With thousands of individual managers predicting sales based on their unique circumstance the accuracy of result can be quite varied.You are provided with historical sales data for 1,115 Rossmann stores.The task is to forecast the 'Sales' column for the test set.Note that some stores in the dataset were temporarily closed for reburisment\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from numpy import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google drive with colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "ross_store=pd.read_csv('/content/drive/MyDrive/Almabetter/Rossmann Stores Data.csv')"
      ],
      "metadata": {
        "id": "RvZibUejQyef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store=pd.read_csv('/content/drive/MyDrive/Almabetter/store(1).csv')"
      ],
      "metadata": {
        "id": "JDQdtpmKRVEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "ross_store.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset first look\n",
        "store.head()"
      ],
      "metadata": {
        "id": "RN1wd4bUWZkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "print(f'Shape of dataset is :{ross_store.shape}')\n",
        "print(f'Our dataset contains {ross_store.index.value_counts().sum()} Rows')\n",
        "print(f'Our dataset contains {ross_store.columns.value_counts().sum()} Columns')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store data set Rows & column count\n",
        "print(f'Shape of dataset is :{store.shape}')\n",
        "print(f'Our dataset contains {store.index.value_counts().sum()} Rows')\n",
        "print(f'Our dataset contains {store.columns.value_counts().sum()} Column')"
      ],
      "metadata": {
        "id": "-uD1ZgPhWl1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ross_store Dataset Info\n",
        "ross_store.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store dataset info\n",
        "store.info()"
      ],
      "metadata": {
        "id": "JfEU3GvEW3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ross_store Dataset Duplicate Value Count\n",
        "ross_store.duplicated().any()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross_store dataset doesn't contain any duplicate value"
      ],
      "metadata": {
        "id": "bDxWqiWbW_d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store Dataset Duplicate Value Count\n",
        "store.duplicated().any()"
      ],
      "metadata": {
        "id": "uaEPrJwhXB33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store dataset doesn't contain any duplicated values"
      ],
      "metadata": {
        "id": "XCsw6almXJAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "ross_store.isnull().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ross_store dataset doesn't contains any null values"
      ],
      "metadata": {
        "id": "7DHnCNNwXckR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(store.isnull().sum().sort_values(ascending=False))\n",
        "print('--'*20)\n",
        "print('Percentage Null Value')\n",
        "print(round(store.isnull().sum().sort_values(ascending=False)*100/len(store),2))"
      ],
      "metadata": {
        "id": "VMlsj_hpXhIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are too many columns which contain null values of high percentage\n",
        "\n",
        "*   Promo2SinceWeek,Promo2SinceYear,PromoInterval,CompetitionOpenSinceMonth,CompetitionOpenSinceYear contains too many null values\n",
        "\n"
      ],
      "metadata": {
        "id": "vSl5hhQLXjHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#describe the dataset\n",
        "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
        "percentiles = [0.01, 0.25, 0.5, 0.75, 0.95,0.99]\n",
        "ross_store.describe(percentiles=percentiles)"
      ],
      "metadata": {
        "id": "WIoTLAaIXyJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some columns which contains outliers like Sales,customers"
      ],
      "metadata": {
        "id": "ufPkJ9iaX1_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#describe dataset\n",
        "store.describe(percentiles=percentiles)"
      ],
      "metadata": {
        "id": "bSw3qaTuX2dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CompetitionDistance column contain outlier"
      ],
      "metadata": {
        "id": "zPQAYD6LYGTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(store.isnull(),cbar=False,yticklabels=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   \n",
        "\n",
        "    We have a dataset of rosmann company.One dataset is roaaman store which contains 1017209 rows and 9 columns.The dataset doesn't contain any null or duplicated values.It gives us inforamation about the type of store,sales in each store by what date through how many number of customers.Also whether the sale was affected by closure of shop on weekends or or any kind of stateholiday or schoolholiday.It also gives on how sales was affected on applying promo\n",
        "*   The other dataset is store which has 1115 Rows and 10 Column.This dataset contains too many null values in different columns.which needs to be taken care in upcoming steps.It provides information like type of store and what is assostment level used in store,how far away is competitor from store,since how long is competitor there in the market and how often is promo applied on the store\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "ross_store.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.columns"
      ],
      "metadata": {
        "id": "PYEVwR_HeDgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Most of the fields are self-explanatory. The following are descriptions for those that aren't. Store - a unique Id that represents store\n",
        "\n",
        "Sales - the turnover for any given day (this is what you are predicting)\n",
        "\n",
        "Customers - the number of customers on a given day\n",
        "\n",
        "Open - whether the store was open or not: 0 = closed, 1 = open\n",
        "\n",
        "StateHoliday - state holiday or not\n",
        "\n",
        "SchoolHoliday - School\n",
        "Holiday or not\n",
        "\n",
        "StoreType - what is the type of the store: a, b, c, d\n",
        "\n",
        "Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
        "\n",
        "CompetitionDistance - distance in meters to the nearest competitor store\n",
        "\n",
        "CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "\n",
        "Promo - indicates whether a store is running a promo on that day\n",
        "\n",
        "Promo2 - store running consecutive promotion or not\n",
        "\n",
        "Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "\n",
        "PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started a new. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_vals =ross_store.apply(lambda col: col.unique())\n",
        "print(unique_vals)\n",
        "print('*'*20)\n",
        "print('Unique Values Count')\n",
        "print(ross_store.apply(lambda col: col.nunique()))"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "state holiday cotains O instead of 0(zero)"
      ],
      "metadata": {
        "id": "oSsCba0gebSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_vals =store.apply(lambda col: col.unique())\n",
        "print(unique_vals)\n",
        "print('*'*40)\n",
        "print('Unique Values Count')\n",
        "print(store.apply(lambda col: col.nunique()))"
      ],
      "metadata": {
        "id": "CrsLoJYgeenP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping coluns from store dataset which contains too many null values\n",
        "store.drop(columns=['PromoInterval','Promo2SinceYear','Promo2SinceWeek','CompetitionOpenSinceYear','CompetitionOpenSinceMonth'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null values in competitiondistance column with median\n",
        "store['CompetitionDistance'].fillna(store['CompetitionDistance'].median(),inplace=True)"
      ],
      "metadata": {
        "id": "LPwyjGVtfdEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both the dataset\n",
        "data=ross_store.merge(store,on='Store',how='left')"
      ],
      "metadata": {
        "id": "Bwz2M5lsffUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first look of the merged dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "3M--NTHTfisZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of the dataset\n",
        "data.shape"
      ],
      "metadata": {
        "id": "es6kkLU_flmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating year,month,week_num column from Date Column\n",
        "data['Date']=pd.to_datetime(data['Date'])\n",
        "data['year']=data['Date'].dt.year\n",
        "data['Month']=data['Date'].dt.month\n",
        "data['week_num']=data['Date'].dt.week"
      ],
      "metadata": {
        "id": "pnnw9WMCf0Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping date column from data\n",
        "data.drop(columns='Date',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "hQ_F2jM5f5rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Outlier Treatment\n"
      ],
      "metadata": {
        "id": "ZCD_9nkIf-f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's deal with the outliers\n",
        "plt.figure(figsize=(10,7))\n",
        "data.boxplot()\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BmUvZMqSgIAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns like Sales,CompetitionDistance and customers contains Outliers but CompetitionDistance contains maximum"
      ],
      "metadata": {
        "id": "odqHWm7HgNPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find out 90,95,99 percentile values of Competition distance column\n",
        "percentiles=[0.90,0.95,0.99]\n",
        "data['CompetitionDistance'].describe(percentiles=percentiles)"
      ],
      "metadata": {
        "id": "pXjfGFi3gQT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this we can see that 99% value is around 36K and maximum value is around 75K,so let's consider only value around 95%"
      ],
      "metadata": {
        "id": "kYxq2qczgUGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find out 90,95,99 percentile values of Sales Column\n",
        "data['Sales'].describe(percentiles=percentiles)"
      ],
      "metadata": {
        "id": "Wh9q_fmcgfP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In sales Feature also outliers is there and let's consider only those value around 95%"
      ],
      "metadata": {
        "id": "g7Om6xUEgoMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find out 90,95,99 percentile values of Customers Column\n",
        "data['Customers'].describe(percentiles=percentiles)"
      ],
      "metadata": {
        "id": "PC-vT91Ggso2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also let's consider value around 1400 that is around 95%"
      ],
      "metadata": {
        "id": "oxvov0fIgxQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's first create copy of our dataset\n",
        "df=data.copy()"
      ],
      "metadata": {
        "id": "wuzzQZdtg06Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape before outlier treatment\n",
        "df.shape"
      ],
      "metadata": {
        "id": "w_sYLuG9g386"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treating Outliers"
      ],
      "metadata": {
        "id": "yuLG0fRTg74y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's consider all the values in which competition distance and sales,Customers values are around 95% percentile\n",
        "df=df[df['CompetitionDistance']<20000]\n",
        "df=df[df['Customers']<1400]\n",
        "df=df[df['Sales']<13000].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "op2xkb67hKKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of the data after the outlier treatment\n",
        "df.shape"
      ],
      "metadata": {
        "id": "-8tHBA6yhNR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's convert stateholiday into state_holiday yes(1) or no(0)\n",
        "df['state_holiday'] = data['StateHoliday'].apply(lambda x: 1 if x in ['a', 'b', 'c'] else 0)"
      ],
      "metadata": {
        "id": "EZPcyMzxhQq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop stateholiday column from the dataset\n",
        "df.drop(columns='StateHoliday',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "QYCiZwbIhTxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   first of all dropped all the columns which includes 'PromoInterval','Promo2SinceYear','Promo2SinceWeek','CompetitionOpenSinceYear','CompetitionOpenSinceMonth' from the store dataset as these contains too many null values\n",
        "\n",
        "*   After that filled null values of competitiondistance column of store dataset with median values\n",
        "\n",
        "*   Then merged both the dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Then created Year,Month,Week_num Columns from the Date column of the dataset and then droped the Date Column\n",
        "\n",
        "\n",
        "*   Then visvalized the outliers of the data using boxplot and it was found out that columns like Competitiondistance,Sales,Customer has maximum outlier\n",
        "\n",
        "*   Then considered around 95% percentile values in order to get rid of some outliers\n",
        "*   Then converted StateHoliday Column into into another column satetholiday in which i consider 0 as No holiday and 1 as Holiday and also dropped StateHoliday Column from the dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 1 Sales Vs DayOfWeek"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was mentioned that some of the shops were closed due to some rebursment so sales during that period was zero,so wel will create a dataframe that filter out those days"
      ],
      "metadata": {
        "id": "A0MMX7qABm6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a dataset where ['Sales']!=0\n",
        "df1=df[df['Sales']!=0]"
      ],
      "metadata": {
        "id": "MDNISQIzBqKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart 1 Sales Per Day\n",
        "#Bivariate Analysis\n",
        "ax=sns.barplot(x=df1['DayOfWeek'],y=df1['Sales'])\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales vs Day of week')\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bLV7g8zdBt1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the barplot it can been seen that after day1-1 there has been decline in the sales.sufficient measures should be taken to look into this matter"
      ],
      "metadata": {
        "id": "P7Q0h9_BB2Gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 2 Sales vs Promo"
      ],
      "metadata": {
        "id": "YT5XG4l6CAUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Sales Vs Promo\n",
        "#Bivariate Analysis\n",
        "ax=sns.barplot(x=df['Promo'],y=df['Sales'])\n",
        "plt.title('Sales Vs Promo')\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1m7c_lB4CBJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has been find out that the sales get's almost double when a promo is running"
      ],
      "metadata": {
        "id": "3eiMlwPwCMVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 3 Sales in Different Months"
      ],
      "metadata": {
        "id": "WNHARLoZCQbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Sales Over Months\n",
        "#Bivariate Analysis\n",
        "ax=sns.lineplot(x=df1['Month'],y=df1['Sales'],marker='o')\n",
        "plt.title('Sales over different Months')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rHV6akA1CTPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can been seen that after october there has a very large growth in sales that might be due to festival season.December shows maximum sales may be because of christmas and new year"
      ],
      "metadata": {
        "id": "ohI-WF4iCYee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 4 Sales in different Stores Over Different Years"
      ],
      "metadata": {
        "id": "QyCMsrV0Cep0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Sales Over Years in Different StoreTypes\n",
        "#Multivariate Analysis\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=df['year'],y=df['Sales'],hue=df['StoreType'])\n",
        "plt.title('Sales Over Years in Different StoreTypes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gs3DmOshChHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was find out that Store b sales has been inceased from 2013 to 2015 and in 2013 store d has maximum sales and in remaining years store b has maximum sales"
      ],
      "metadata": {
        "id": "bi48VbBeCnp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 5 Sales vs School Holiday"
      ],
      "metadata": {
        "id": "KJbn579zCr8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Sales vs SchoolHoliday\n",
        "#Bivaraite Analysis\n",
        "ax=sns.barplot(x=df['SchoolHoliday'],y=df['Sales'])\n",
        "plt.title('Sales vs SchoolHoliday')\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6eErVWdPCvdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph it can be seen that there is a very good sales when schoolholiday is there, also good sales can be seen when there is no school Holiday,so SchoolHoliday doesn't really affect the sales"
      ],
      "metadata": {
        "id": "QszmoUDtC0Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Chart - 6"
      ],
      "metadata": {
        "id": "tlPh9sBRC1yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - sales vs StateHoliday\n",
        "#Bivariate Analysis\n",
        "#plotting barplot\n",
        "ax=sns.barplot(x=df['state_holiday'],y=df['Sales'])\n",
        "plt.title('Sales vs StateHoliday')\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "plt.show()\n",
        "#plotting pie chart\n",
        "df['state_holiday'].value_counts().plot(kind='pie',autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I2QiTBO7C8zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the barplot it can be easily seen that it doesn't matter whether there is state holiday or not there is good amount of sales on both the occassions and from the pie chart it can be seen that only 3% of sales is effected by stateholiday.So overall sales doesn't depend on stateholiday"
      ],
      "metadata": {
        "id": "p_yNqqNvDAsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 7 Sales vs Assostment Level"
      ],
      "metadata": {
        "id": "4nO-fzP1DHa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Sales vs AssortmentLevel in all stores\n",
        "#Multivaraite Analysis\n",
        "ax=sns.barplot(x=df['StoreType'],y=df['Sales'],hue=df['Assortment'])\n",
        "plt.title('Sales Vs Assortment Level ')\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NAB3kCK2DI6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "From the plot it can that in Store B only a and b assortment has been used and out of which assortment a got maximum sales\n",
        "\n",
        "*   Also in remaining Store only a and c assortment level has been and they got equal sales\n",
        "\n"
      ],
      "metadata": {
        "id": "d4iCCg4RDTK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 8 Sales Vs Customer"
      ],
      "metadata": {
        "id": "Y3TWzhcsDefu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Relation between Sales and Customer\n",
        "#Bivariate Analysis\n",
        "sns.scatterplot(x=df['Customers'],y=df['Sales'])\n",
        "plt.title('Sales vs Customer')"
      ],
      "metadata": {
        "id": "Daj1uyjSDhYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was find out from the plot that the customers and sales are positively corelated to each other i,e more customer visits the store more will be sales"
      ],
      "metadata": {
        "id": "mUOkxaBeDkxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 9"
      ],
      "metadata": {
        "id": "nqf0MZHbEDpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#Bivariate Analysis\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.scatterplot(x=df['CompetitionDistance'],y=df['Sales'])\n",
        "plt.title('Sales vs CompetitionDistance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZrPaLFYEIHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can been seen that most of the stores are very desely located to each other have more sales"
      ],
      "metadata": {
        "id": "_PPuU_OdEN_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 10 Sales Vs Promo 2"
      ],
      "metadata": {
        "id": "gRwPOBegESl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "#Bivariate Analysis\n",
        "ax=sns.barplot(x=df['Promo2'],y=df['Sales'])\n",
        "plt.title('Sales vs Promo 2')\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OvQYYZtHEXc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen from the graph that Running promo continously has not been that much effective towards increasing Sales"
      ],
      "metadata": {
        "id": "VYicBFY0Ee0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 11 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "qEPoVGUhEkpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,5))\n",
        "corr=df.corr()\n",
        "sns.heatmap(corr,annot=True)"
      ],
      "metadata": {
        "id": "oXCaORqLEpxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   \n",
        "\n",
        "    It can be find out that Promo,Open and customer has positive correlation with Sales means that if the shop is ruuning any promo more customers will visit the store there will be more sales\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "*   Day of week is in negative correlation with sales that means on weekends Store will be closed,there will be no sales and competiton distance also some negative correlation.\n",
        "*   State holiday has very low negative correlation with sales as sales is not affected by state holiday same case with the school holiday\n",
        "\n",
        "\n",
        "*   Multicolinearity lies between promo,customers and open column\n",
        "\n"
      ],
      "metadata": {
        "id": "8TFnDIOmEuru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "s4kh1gTTFSXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "gXj22lT6FXCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph it can be seen what all are the features that are positively corelated with sales"
      ],
      "metadata": {
        "id": "-Z9VdQ1IFdet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.\n",
        "\n",
        "\n",
        "*   \n",
        "\n",
        "    Null Hypothesis: There is no relationship between Sales and Customers\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*    Alternative Hypothesis: There is relationship between Sales and Customers\n",
        "\n",
        "*   \n",
        "\n",
        "    Null Hypothesis: There is no relationship between Sales and Promo\n",
        "\n",
        "*   Alternative Hypothesis:There is relationship between Sales and Promo\n",
        "\n",
        "\n",
        "*   Null Hypothesis: There is no relationship between Sales and DayOfWeek\n",
        "\n",
        "\n",
        "*    Alternative Hypothesis: There is relationship between Sales and DayOfWeek\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   \n",
        "\n",
        "    Null Hypothesis: There is no relationship between Sales and Customers\n",
        "    \n",
        "\n",
        "*   ALternative Hypothesis: There is relationship between Sales and Customers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "sales = df['Sales']\n",
        "customers = df['Customers']\n",
        "\n",
        "correlation, p_value = pearsonr(sales, customers)\n",
        "\n",
        "print(\"Pearson correlation coefficient:\", correlation)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant relationship between Sales and Customers.\")\n",
        "else:\n",
        "    print(\"Accept the null hypothesis. There is no significant relationship between Sales and Customers.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find out the correlation coefficient and P vlaue between the two features,I am using Pearson method.It helps me to find out the Corelation coeficient value which ranges from -1 and 1. A Correlation value of 1 means perfect positive correlation, whereas correlation value of -1 means strong negative relationship and a correlation value of 0 means no relationship between the features. Also the P-value helps us to find out the statistical importance of the correlation.If the p-value is less than singnificance value which is generally taken as 0.05 indicates that the correlation is significant and provides us evidence to reject the null hypothesis."
      ],
      "metadata": {
        "id": "gBhFvE7uXv8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Null Hypothesis: There is no relationship between Sales and Promo\n",
        "\n",
        "\n",
        "\n",
        "*   ALternative Hypothesis:There is relationship between Sales and Promo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "sales = df['Sales']\n",
        "promo = df['Promo']\n",
        "\n",
        "correlation, p_value = pearsonr(sales, promo)\n",
        "\n",
        "print(\"Pearson correlation coefficient:\", correlation)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant relationship between Sales and Promo.\")\n",
        "else:\n",
        "    print(\"Accept the null hypothesis. There is no significant relationship between Sales and Promo.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find out the correlation coefficient and P vlaue between the two features,i am using Pearson method.It helps me to find out the Corelation coeficient value which ranges from -1 and 1. A Correlation value of 1 means perfect positive correlation, whereas correlation value of -1 means strong negative relationship and a correlation value of 0 means no relationship between the features. Also the P-value helps us to find out the statistical importance of the correlation.If the p-value is less than singnificance value which is generally taken as 0.05 indicates that the correlation is significant and provides us evidence to reject the null hypothesis"
      ],
      "metadata": {
        "id": "wcKy9SzoYUVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Null Hypothesis: There is no relationship between Sales and SchoolHoliday\n",
        "\n",
        "ALternative Hypothesis: There is relationship between Sales and SchoolHoliday\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "sales = df['Sales']\n",
        "Schoolholiday= df['SchoolHoliday']\n",
        "\n",
        "correlation, p_value = pearsonr(sales, Schoolholiday)\n",
        "\n",
        "print(\"Pearson correlation coefficient:\", correlation)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant relationship between Sales and SchoolHoliday .\")\n",
        "else:\n",
        "    print(\"Accept the null hypothesis. There is no significant relationship between Sales and SchoolHoliday.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find out the correlation coefficient and P vlaue between the two features,i am using Pearson method.It helps me to find out the Corelation coeficient value which ranges from -1 and 1. A Correlation value of 1 means perfect positive correlation, whereas correlation value of -1 means strong negative relationship and a correlation value of 0 means no relationship between the features. Also the P-value helps us to find out the statistical importance of the correlation.If the p-value is less than singnificance value which is generally taken as 0.05 indicates that the correlation is significant and provides us evidence to reject the null hypothesis."
      ],
      "metadata": {
        "id": "dkrE6dLTYo4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#info of the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "8abbyYY00CkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's change the datatype of CompetitionDistance to int\n",
        "df['CompetitionDistance']=df['CompetitionDistance'].astype('int')"
      ],
      "metadata": {
        "id": "z4FAOnzq0U37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "oW8PSvRy0ZN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data frame is already cleaned.All the missing values has already filled during EDA Process"
      ],
      "metadata": {
        "id": "HIa_G6FK0fbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers can lead to biased insights and conclusions when analyzing data.So we should treat the outliers before passing the data into our model.In our dataset only in Sales,Customers and CompetitionDistnace outliers were present and i have already removed them during EDA process by consideing values around 95 percentile and removed all other values above it"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StoreType,Assortment Column Contains Categorical Values So, i have to Encode them"
      ],
      "metadata": {
        "id": "eqm_lnu-02vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "#Encode StoreType,Assortment column using One Hot Encoding\n",
        "encoded_df = pd.get_dummies(df, columns=['StoreType','Assortment'],dtype=int,drop_first=True)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#head of the data after categorical encoding\n",
        "encoded_df.head()"
      ],
      "metadata": {
        "id": "qalSlKQt1LJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In our dataframe StoreType,Assortment are Cetagorical Features,so inorder to encode these features i have used get_dummies Function from pandas for One Hot Encoding as it is straight forward and easy to use function provided by pandas to perform one hot encoding in just one line of code and also it retains the original column names and allows for easy interpretation and exploration of the encoded categorical variables\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "#let's calculate the VIF to find Multicolinearity between the features\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "X=encoded_df[['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo',\n",
        "       'SchoolHoliday', 'CompetitionDistance', 'Promo2', 'year', 'Month',\n",
        "       'week_num', 'state_holiday', 'StoreType_b', 'StoreType_c',\n",
        "       'StoreType_d', 'Assortment_b', 'Assortment_c']]\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = X.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here many of the features have very high Multicolinearity so we have to exclude some feature\n",
        "\n",
        "*   Month have very high corelation with week number i,e 0.97 and their respective corelation with sales is 0.027 and 0.032,so we will keep Week_no feature\n",
        "*   \n",
        "Year feature is has also high VIF, So we remove this also\n",
        "\n",
        "*   Customer and open also have high VIF value and also correlation with each other is 0.77 and with sales is 0.91 and 0.77 respectively so we will drop open column\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N7DpM0Lc1zN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After Analysing all feature and Removing those features having high corelation\n",
        "#calculating VIF again\n",
        "X=encoded_df[['Store', 'DayOfWeek', 'Customers', 'Promo',\n",
        "       'SchoolHoliday', 'CompetitionDistance', 'Promo2', 'Month',\n",
        "        'state_holiday', 'StoreType_b', 'StoreType_c',\n",
        "       'StoreType_d', 'Assortment_b', 'Assortment_c']]\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = X.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "oHaEoQme2c2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here VIF is now less than 5 for all features"
      ],
      "metadata": {
        "id": "JS_SlTXn2f0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only those features which are important\n",
        "new_df=encoded_df[[ 'DayOfWeek', 'Customers', 'Promo',\n",
        "       'SchoolHoliday', 'CompetitionDistance', 'Promo2', 'Month',\n",
        "        'state_holiday', 'StoreType_b', 'StoreType_c',\n",
        "       'StoreType_d', 'Assortment_b', 'Assortment_c','Sales']]\n"
      ],
      "metadata": {
        "id": "j0N0Sa0a2lBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "zsV8a2s-2pPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   \n",
        "\n",
        "    I have used heatmap to find out the corelation of all the features with each other and with sales also and it was find out that some of the features have very positive,negative and very minimum corelation with sales.Customers,open and sales have very high corelation with sales and dayofweek has low\n",
        "*   \n",
        "After that i calculated VIF value of all the features with each other and excluded those features which were having Multicolinearity\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features like 'DayOfWeek', 'Customers', 'Promo','SchoolHoliday', 'CompetitionDistance', 'Promo2', 'Month','is_state_holiday', 'StoreType_b', 'StoreType_c','StoreType_d', 'Assortment_b', 'Assortment_c','Sales' are important feature.These are having VIF under 5 and corelation with sales and also each feature is important for predicting Sales"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's create a dataframe where sales!=0\n",
        "new_df1=new_df[new_df['Sales']!=0]"
      ],
      "metadata": {
        "id": "rOKzsMih3Gr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we know that there are some values where sales is zero because shop was closed due to rebursment."
      ],
      "metadata": {
        "id": "7Kkmr_I13KAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find out skewness of continous feature\n",
        "new_df1[['Customers','CompetitionDistance','Sales']].skew()"
      ],
      "metadata": {
        "id": "6ygMsrTj3M_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In customers,competitionsDistance there is positive skewness present,Now let's transform the data\n"
      ],
      "metadata": {
        "id": "bxCk1JKp3N9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find out skewness of continous feature\n",
        "columns=['Customers','CompetitionDistance','Sales']\n",
        "for col in columns:\n",
        "  sns.distplot(new_df1[col])\n",
        "  plt.axvline(new_df1[col].mean(), color='red', linestyle='--', label=f'Mean: {new_df1[col].mean():.2f}')\n",
        "  plt.axvline(new_df1[col].median(), color='green', linestyle='--', label=f'Median: {new_df1[col].median():.2f}')\n",
        "  plt.axvline(new_df1[col].mode()[0], color='blue', linestyle='--', label=f'Mode: {new_df1[col].mode()[0]:.2f}')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "CyGcfwJj3XAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that each feature is rightly skewed,so let's apply sqrt tranformation."
      ],
      "metadata": {
        "id": "4d0tqgS23gSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TransFrom the data using SQRT transformation\n",
        "new_df1['CompetitionDistance']=np.sqrt(new_df1['CompetitionDistance'])\n",
        "new_df1['Customers']=np.sqrt(new_df1['Customers'])\n",
        "new_df1['Sales']=np.sqrt(new_df1['Sales'])"
      ],
      "metadata": {
        "id": "A1RqnFEZ3jcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#before Scaling let's find out dependent and independent features\n",
        "X=new_df1.drop(columns=['Sales']) #idependent Variable/feature\n",
        "y=new_df1[['Sales']]                #Dependent Variable"
      ],
      "metadata": {
        "id": "_Zcz-QxS35kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "I86ears738Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "#fit and transform\n",
        "X=sc.fit_transform(X)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used StandardScaler from the scikit-learn library for scaling data. This method transforms the data such that it has a mean of 0 and a standard deviation of 1.It helps to bring all features onto a similar scale.When features have significantly different scales, it can lead to biased model training or result in features with larger scales dominating the learning process."
      ],
      "metadata": {
        "id": "aYkNi8O45iyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As dataset is huge i have choosen 80-20 ratio. By allocating 80% of the data to the training set, we will get enough data to train the model and capture patterns and relationships in the data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "dYNMafixjeD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr=LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lr.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_train_predict=lr.predict(x_train)\n",
        "y_test_predict=lr.predict(x_test)"
      ],
      "metadata": {
        "id": "OfL0S6pIjmOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coefficients\n",
        "lr.coef_"
      ],
      "metadata": {
        "id": "9EU6djsNjpfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intercept\n",
        "lr.intercept_"
      ],
      "metadata": {
        "id": "mwCcsMaJjszr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing labraries to check the accuracy of the model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "iZoe5KJdj94w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's define a function of metrics\n",
        "def cal_metrics_score(y_train,y_train_pred,y_test,y_test_pred):\n",
        "  #mean_squared_error metrics\n",
        "  MSE_train=mean_squared_error(y_train,y_train_pred)\n",
        "  print('MSE train score is :',MSE_train)\n",
        "  MSE_test=mean_squared_error(y_test,y_test_pred)\n",
        "  print('MSE test score is :',MSE_test)\n",
        "\n",
        "  #Root mean square error\n",
        "  print('RMSE train Score: ',np.sqrt(MSE_train))\n",
        "  print('RMSE test Score: ',np.sqrt(MSE_test))\n",
        "\n",
        "  #R2 score\n",
        "  r2_train=r2_score(y_train,y_train_pred)\n",
        "  print('r2 train score: ',r2_train)\n",
        "  r2_test=r2_score(y_test,y_test_pred)\n",
        "  print('r2 test score: ',r2_test)"
      ],
      "metadata": {
        "id": "RsXkLgIskZkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating metrics\n",
        "cal_metrics_score(y_train,y_train_predict,y_test,y_test_predict)"
      ],
      "metadata": {
        "id": "705bwxMckdEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot residuals\n",
        "residuals=y_test-y_test_predict\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(y_test,residuals)\n",
        "plt.axhline(y=np.nanmean(residuals), color='red', linestyle='--',label=round(np.mean(residuals),2)[0])\n",
        "plt.axhline(y=np.nanmedian(residuals), color='black', linestyle='--',label=round(np.median(residuals),2))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EICCP4t7kqp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "linear regression model achieved a reasonably good R-squared score of 81.2%, indicating that it explains a significant portion of the variance in the target variable and has low MSE,RMSE on both trainig and test sample.The residual plot, which is a graphical representation of the differences between the actual values (y_test) and the predicted values (y_test_predict) was also plotted.It helps us to visvalize the distribution of residuals.The residuals shows that residuls are scattered around mean or zero line that indicates the model presiction is unbiased.\n",
        "\n",
        "*   since the r2 score is not that let's go for cross validation and hyperparameter tunning\n",
        "\n"
      ],
      "metadata": {
        "id": "hX6trLO8kvn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression"
      ],
      "metadata": {
        "id": "RVoq8aAhlHRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Fit the Algorithm\n",
        "ridge = Ridge()\n",
        "param_grid = {'alpha': [0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(ridge, param_grid,scoring='r2',cv=5)\n",
        "grid_search.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_pred = grid_search.predict(x_train)\n",
        "y_test_pred = grid_search.predict(x_test)\n",
        "\n",
        "# Retrieve the best hyperparameters and best score\n",
        "best_params = print('best param:',grid_search.best_params_)\n",
        "best_score = print('best score:',grid_search.best_score_)"
      ],
      "metadata": {
        "id": "6z2CuxhslJQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating metrics for Ridge\n",
        "cal_metrics_score(y_train,y_train_pred,y_test,y_test_pred)"
      ],
      "metadata": {
        "id": "kaoigf7tlQAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Ridge Regression has shown same performance as of Linear Regression\n"
      ],
      "metadata": {
        "id": "iBoeGta8lUBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression"
      ],
      "metadata": {
        "id": "kM9JO8LFlY50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Fit the Algorithm\n",
        "lasso = Lasso()\n",
        "param_grid = {'alpha': [0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(lasso, param_grid,scoring='r2',cv=5)\n",
        "grid_search.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_lasso_pred = grid_search.predict(x_train)\n",
        "y_test_lasso_pred = grid_search.predict(x_test)\n",
        "\n",
        "# Retrieve the best hyperparameters and best score\n",
        "best_params = print('best param:',grid_search.best_params_)\n",
        "best_score = print('best score:',grid_search.best_score_)"
      ],
      "metadata": {
        "id": "U4ytYuXCldl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating metrics for Lasso Regressions\n",
        "cal_metrics_score(y_train,y_train_lasso_pred,y_test,y_test_lasso_pred)"
      ],
      "metadata": {
        "id": "73RGeuMflhnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression has also shown same result there is no improvement in performance"
      ],
      "metadata": {
        "id": "XBYnkIR3llz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net"
      ],
      "metadata": {
        "id": "gwTqCLYmlnR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Fit the Algorithm\n",
        "enet= ElasticNet()\n",
        "param_grid = {'alpha': [0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(enet, param_grid,scoring='r2',cv=5)\n",
        "grid_search.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_enet_pred = grid_search.predict(x_train)\n",
        "y_test_enet_pred = grid_search.predict(x_test)\n",
        "\n",
        "# Retrieve the best hyperparameters and best score\n",
        "best_params = print('best param:',grid_search.best_params_)\n",
        "best_score = print('best score:',grid_search.best_score_)"
      ],
      "metadata": {
        "id": "gWlFz9ollxaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating metrics for ElasticNet Regression\n",
        "cal_metrics_score(y_train,y_train_enet_pred,y_test,y_test_enet_pred)"
      ],
      "metadata": {
        "id": "5MOYmuTdl0cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net has also shown around same results"
      ],
      "metadata": {
        "id": "JRMJHUXxl7ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "ISvTFVtsl_rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "I have used GridSearchCV technique for hyperparameter tuning . It systematically searches through a predefined set of hyperparameters and evaluates the model's performance for each combination using cross-validation\n"
      ],
      "metadata": {
        "id": "q4rW5XyTmDGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GiMV0c8umGIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried multiple regularization techniques, including Lasso, Ridge, and Elastic Net, with Gridsearch cv as a hyperparameter. However, i observed that the R-squared score remains the same despite these attempts.Now let's try some complex Models"
      ],
      "metadata": {
        "id": "ll2h_dDtmJyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "5MEfGaC_mUxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dt=DecisionTreeRegressor(max_depth=20)\n",
        "# Fit the Algorithm\n",
        "dt.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_train_dt_pred=dt.predict(x_train)\n",
        "y_test_dt_pred=dt.predict(x_test)\n"
      ],
      "metadata": {
        "id": "cHJQRUPLmb10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate evaluation Metric Score\n",
        "cal_metrics_score(y_train,y_train_dt_pred,y_test,y_test_dt_pred)"
      ],
      "metadata": {
        "id": "60dEor06mete"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot residuals\n",
        "y_test_dtt_pred= y_test_dt_pred.reshape(-1,1)\n",
        "residuals=y_test-y_test_dtt_pred\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.scatter(y_test,residuals)\n",
        "plt.axhline(y=np.nanmean(residuals), color='red', linestyle='--',label=round(np.mean(residuals),2)[0])\n",
        "plt.axhline(y=np.nanmedian(residuals), color='black', linestyle='--',label=round(np.median(residuals),2))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XGaijKycmh4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The decision tree model with max_depth of 20 has achieved high performance on both the training and test data with high R2 score of 97.67% on the training data and 93.18% on the test data.The Model has achieved a relatively low MSE and RMSE value of 13.68,3.69 respectively which predicts average magnitude errors made by the decision tree model.The residuals plot shows that residuls are scattered around mean or zero line that indicates the model presiction is unbiased.\n",
        "\n",
        "Overall, the decision tree model demonstrates strong predictive performance, with high R2 scores, accurate predictions indicated by mean and median residuals, and relatively low MSE and RMSE scores.\n",
        "\n",
        "Let's now apply hyperparameter tunning to find best parameters for the model for best performace\n"
      ],
      "metadata": {
        "id": "u-AkycZZmqll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Fit the Algorithm\n",
        "dt=DecisionTreeRegressor()\n",
        "param_grid={'max_depth': [17,18,19,20], 'min_samples_leaf': [5,6,7,8], 'min_samples_split': [1,3,4,5]}\n",
        "gridsearch=GridSearchCV(dt,param_grid,scoring='r2', cv=3)\n",
        "gridsearch.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_train_dtgs_pred=gridsearch.predict(x_train)\n",
        "y_test_dtgs_pred=gridsearch.predict(x_test)\n",
        "# Retrieve the best hyperparameters and best score\n",
        "best_params = print('best param:',gridsearch.best_params_)\n",
        "best_score = print('best score:',gridsearch.best_score_)"
      ],
      "metadata": {
        "id": "8NfGZR1qmyoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate evaluation Metric Score\n",
        "cal_metrics_score(y_train,y_train_dtgs_pred,y_test,y_test_dtgs_pred)"
      ],
      "metadata": {
        "id": "-ohDHcR0m244"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "I have used GridSearchCV technique for hyperparameter tuning . It systematically searches through a predefined set of hyperparameters and evaluates the model's performance for each combination using cross-validation and gives out best possible combination of the parameters\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing hyperparameter tuning using grid search CV and cross-validation, the decision tree model was further optimized. It was founded that the best comination of hyperparameteres was max_depth of 20 and min_sample_leaf of 5 and min_samples_split was found to be 3.With all these combination we have achieved a optimal model"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "98xy9h5QnZEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf=RandomForestRegressor(n_estimators=100,max_depth=20)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_rf_pred=rf.predict(x_train)\n",
        "y_test_rf_pred=rf.predict(x_test)"
      ],
      "metadata": {
        "id": "_FYN6Qx1nr6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate evaluation Metric Score\n",
        "cal_metrics_score(y_train,y_train_rf_pred,y_test,y_test_rf_pred)"
      ],
      "metadata": {
        "id": "WtfgGkINnvL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot residuals\n",
        "y_test_rff_pred= y_test_rf_pred.reshape(-1,1)\n",
        "residuals=y_test-y_test_rff_pred\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.scatter(y_test,residuals)\n",
        "plt.axhline(y=np.nanmean(residuals), color='red', linestyle='--',label=round(np.mean(residuals),2)[0])\n",
        "plt.axhline(y=np.nanmedian(residuals), color='black', linestyle='--',label=round(np.median(residuals),2))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w4YgO-29n24p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "After Decision Tree i have now applied the Random Forest model with 100 estimators and a maximum depth of 20.The model has performed pretty good on both training and test data sample with high r2 score of 98% on training and 95.6% on test sample suggesting that the model performs well in explaining the variance in the target variable for unseen dataand low MSE and RMSE of 8.7,2.96 respectively on test sample.The residuals plot has also shown that residuals are centered around zero.\n",
        "\n",
        "In summary, the model performance is very good with n_estimator 0f 100 and with max_depth of 20 and gives out good r2_score\n",
        "\n",
        "Let's hypertune this model to find optimal parameters for optimal model\n"
      ],
      "metadata": {
        "id": "k1hnIMaqn-tN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Fit the Algorithm\n",
        "rf=RandomForestRegressor()\n",
        "param_grid={'n_estimators':[100],'max_depth':[18,19,20]}\n",
        "rf_randomsearch=RandomizedSearchCV(rf,param_grid,scoring='neg_mean_squared_error',n_iter=3,n_jobs=-1,cv=3,verbose=2)\n",
        "rf_randomsearch.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_train_rffgs_pred=rf_randomsearch.predict(x_train)\n",
        "y_test_rfgs_pred=rf_randomsearch.predict(x_test)\n",
        "# Retrieve the best hyperparameters and best score\n",
        "best_params = print('best param:',rf_randomsearch.best_params_)\n",
        "best_score = print('best score:',rf_randomsearch.best_score_)"
      ],
      "metadata": {
        "id": "CkROzXTroFA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate evaluation Metric Score\n",
        "cal_metrics_score(y_train,y_train_rffgs_pred,y_test,y_test_rfgs_pred)"
      ],
      "metadata": {
        "id": "b8HEVGLToJic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Random Search cv hyperparameter to tune my model.It is more efficient than GridSearchCV when searching through a large hyperparameter space because it does not evaluate all possible combinations."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying Random Search cv as a hyperparamter tunning it was found that the best parametes are n_estimator=100 and max_depth=20.The model gave out almost same MSE,RMSE and R2 score"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4"
      ],
      "metadata": {
        "id": "ZoynTkdyxn5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBOOST"
      ],
      "metadata": {
        "id": "uuqb-k9Lxqg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the algorithm\n",
        "from xgboost import XGBRegressor\n",
        "xgb = XGBRegressor(n_estimators=100,learning_rate=0.1, max_depth=13)\n",
        "#fit the algorithm\n",
        "xgb.fit(x_train,y_train)\n",
        "# Predict on the mode\n",
        "y_train_xgb_pred=xgb.predict(x_train)\n",
        "y_test_xgb_pred=xgb.predict(x_test)"
      ],
      "metadata": {
        "id": "JUu0b5PNx5mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate evaluation Metric Score\n",
        "cal_metrics_score(y_train,y_train_xgb_pred,y_test,y_test_xgb_pred)"
      ],
      "metadata": {
        "id": "3cSmxdbSx8a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot residuals\n",
        "y_test_xgbb_pred= y_test_xgb_pred.reshape(-1,1)\n",
        "residuals=y_test-y_test_xgbb_pred\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.scatter(y_test,residuals)\n",
        "plt.axhline(y=np.nanmean(residuals), color='red', linestyle='--',label=round(np.mean(residuals),2)[0])\n",
        "plt.axhline(y=np.nanmedian(residuals), color='black', linestyle='--',label=round(np.median(residuals),2))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C1568kaTyAFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying XGBoost with 100 estimators, a learning rate of 0.1, and a maximum depth of 13, the model has shown MSE train of 6.05,and the RMSE train score is 2.46, which is the square root of the MSE train score. These scores represent the model's performance on the training data.similarly the MSE test score is 7.89,and the RMSE test score is 2.8. These scores indicate the model's performance on unseen data.\n",
        "\n",
        "The Model has shown R2 score of 96.97 on training and 96.07 on test, indicating that the model explains approximately 96% of the variability in the target variable.\n",
        "The scatter plot also shows residual mean of 0 and residual vlaue of -0.09.\n",
        "\n",
        "In summary, the XGBoost model with 100 estimators, a learning rate of 0.1, and a maximum depth of 13 demonstrates good performance, with relatively low MSE and RMSE scores and high R2 scores on both the training and test data.\n",
        "\n",
        "Let's hypertune the model for best performance and best parameters"
      ],
      "metadata": {
        "id": "6AQ_xOSHyKGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4i0W62LNyPLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost with RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating XGBoost instance\n",
        "xgb= XGBRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters={\"learning_rate\":[0.01, 0.1],\"max_depth\":[13,14,15]}\n",
        "\n",
        "# Train the model\n",
        "xgb_grid=GridSearchCV(xgb,parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "xgb_grid.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_xgbg_pred = xgb_grid.predict(x_train)\n",
        "y_test_xgbg_pred = xgb_grid.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print('best param',xgb_grid.best_params_)\n",
        "print('best score',xgb_grid.best_score_)"
      ],
      "metadata": {
        "id": "KwEs5V_KyTEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate evaluation Metric Score\n",
        "cal_metrics_score(y_train,y_train_xgbg_pred,y_test,y_test_xgbg_pred)"
      ],
      "metadata": {
        "id": "7Z9aXzjZyWb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying Grid Search cv as hypertune and cross validation parameter it was find out that the best parameter are learning rate of 0.1 and max_depth of 15 and with 100 estimators, a learning rate of 0.1, and a maximum depth of 15 demonstrates good performance, with relatively low MSE and RMSE scores and high R2 scores on both the training and test data. The model's predictions are generally accurate, as indicated by the mean and median residual values close to zero."
      ],
      "metadata": {
        "id": "w-4OZKpgybxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "AB_xl7lOyfOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Consider several key metrics that assess the performance of machine learning models.\n",
        "\n",
        "One essential metric is the R2 score, which is also known as the coefficient of determination. It hepls ud to measure how much of the target variable's variance the model can explain.A higher R2 score indicates a stronger relationship between the features and the target variable which is very important for accurate prediction of the result.\n",
        "\n",
        "Another important metric is the Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values. A lower MSE suggests smaller prediction errors and higher accuracy in the model's predictions. Minimizing prediction errors is critical for businesses as it helps us to improves the reliability of decision-making.\n",
        "\n",
        "The next metric is Root Mean Squared Error (RMSE) it also provides an average measure of the prediction errors in the original unit of the target variable. Similar to MSE, a lower RMSE indicates better prediction accuracy.\n",
        "\n",
        "Also analyzing the residuals, that is the differences between the predicted and actual values, can also provide us valuable insights. A well-behaved residual plot with a mean close to zero and no discernible patterns suggests that the model effectively captures the underlying patterns.\n",
        "\n",
        "Considering these evaluation metrics allows businesses to assess the model's performance, accuracy, and reliability.\n"
      ],
      "metadata": {
        "id": "C4nrSLb8ym0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "g9wxiWz-ys8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Based on the performance of all the models, i have selected XGBoost model as the final prediction model. The XGBoost model with learning rate of 0.1 and max_depth of 15 and 100 estimators, outperformed the other models in several key aspects, making it the preferred choice for prediction.\n",
        "\n",
        "First of all the XGBoost model achieved high r2 score of around 98% on the training data and r2 score of around 97% in test dataset\n",
        "\n",
        "also the XGBoost model achieved lower Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) values compared to the other models. With an MSE of 4.7, and an RMSE of 2.7, the XGBoost model exhibits smaller prediction errors and improved accuracy in predicting the target variable.\n",
        "\n",
        "As the XGBoost model displayed consistent performance across various evaluation metrics, including the R2 score, MSE, and RMSE. Its high performance on both the training and test data indicates good capabilities and reduces the risk of overfitting.\n",
        "\n",
        "Additionally the residuals analysis revealed mean and median values of 0 and -.09, respectively.The well-behaved residuals indicate that the XGBoost model effectively captures the underlying patterns in the data and minimizes systematic errors.\n",
        "\n",
        "Considering the model's strong predictive performance, lower prediction errors, and consistency across evaluation metrics, the XGBoost model is the preferred choice as the final prediction model.\n"
      ],
      "metadata": {
        "id": "rA8PgLLTyzmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "kpwwxmP5y6bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The Model can be explained in two ways:\n",
        "\n",
        "1.   Globally :\n",
        "\n",
        "\n",
        "\n",
        "This is the overall explanation of model behavior. It shows us a big picture view of the model, and how features in the data collectively affect the result. e.g Feature importance\n",
        "\n",
        "\n",
        "2.   Locally :\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "This tells us about each instance and feature in the data individually (kind of like explaining observations seen at certain points in the model), and how features individually affect the result.e.g Lime ,Shap\n"
      ],
      "metadata": {
        "id": "6XDl1Itvy_X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the barplot to find out which feature is contributing the most\n",
        "from xgboost import XGBRegressor\n",
        "model = XGBRegressor(n_estimators=100,learning_rate=0.1, max_depth=15)\n",
        "# Train your XGBoost model\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "# Get the feature importance scores\n",
        "importance = model.feature_importances_\n",
        "\n",
        "indices = np.argsort(importance)"
      ],
      "metadata": {
        "id": "yqdPyn_gzbUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=new_df.columns\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importance[indices], color='red', align='center')\n",
        "\n",
        "# Add numeric values on top of the bars\n",
        "for i, v in enumerate(importance[indices]):\n",
        "    plt.text(v, i, f'{v:.4f}', color='black', va='center')\n",
        "\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTBPcgrIzfHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have applied Feature importance method to find out what are the features that are important for the prediction of the target feature:\n",
        "\n",
        "It was found that Assortment b is contributing the most around 45.6,followed by the store_type_d and then by storetype_b, and then by promo and customers and so on."
      ],
      "metadata": {
        "id": "wJ3TqUeTzkjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA Conclusion"
      ],
      "metadata": {
        "id": "WbVp_61Pnq6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA helps us to extract some useful insights from the data.some of the insights drawn from the data are as:\n",
        "\n",
        "*   It has been find out that Day-1 i,e Monday has got maximum Sales and Day-7 i,e Sunday has got least\n",
        "\n",
        "*   When any kind of Promo was applied ,during that period sales got almost doubles\n",
        "\n",
        "*   After october there was a very large growth in sales that might be due to festival season.December shows maximum sales may be because of christmas and new year\n",
        "\n",
        "*   It was find out that Store b sales has been inceased from 2013 to 2015 and in 2013 store d has maximum sales and in remaining years store b has maximum sales\n",
        "\n",
        "*   Sales are highest for the assortment b.\n",
        "\n",
        "*   It was also observed that mostly the competitor stores weren't that far from each other and the stores densely located near each other saw more sales.\n",
        "*   School Holiday doesn't really affected sales but there little more sales during school Holiday Period\n",
        "\n",
        "\n",
        "*   Whether state holiday or not there was good amount of sales on both the occassions and it was find out that only 3% of sales is effected by stateholiday.So overall sales doesn't depend on stateholiday\n",
        "\n",
        "\n",
        "*   \n",
        "The customers and sales are positively corelated to each other i,e more customer visits the store more will be sales\n",
        "\n",
        "\n",
        "*   It was observed that Running promo continously has not been that much effective towars increasing Sales\n",
        "\n",
        "\n",
        "*   Promo,Open and customer has positive correlation with Sales means that if the shop is ruuning any promo more customers will visit the store there will be more sales\n",
        "\n",
        "\n",
        "*   State holiday has very low negative correlation with sales as sales is not affected by state holiday same case with the school holiday\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Conclusion"
      ],
      "metadata": {
        "id": "cys00Xijpeup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "I have implemented Linear Regression,Ridge Regression,Lasso Regression,Decision Tree,Random Forest and XGBoost model on my data. Based on the performance of all the models, i have selected XGBoost model as the final prediction model. The XGBoost model outperformed the other models in several key aspects, making it the preferred choice for prediction.\n",
        "\n",
        "The XGBoost model with learning rate of 0.1 and max_depth of 15 and 100 estimators, outperformed the other models in several key aspects, making it the preferred choice for prediction.\n",
        "\n",
        "First of all the XGBoost model achieved high r2 score of around 98% on the training data and r2 score of around 97% in test dataset\n",
        "\n",
        "The XGBoost model achieved lower Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) values compared to the other models. With an MSE of 4.7, and an RMSE of 2.7, the XGBoost model exhibits smaller prediction errors and improved accuracy in predicting the target variable.\n",
        "\n",
        "As the XGBoost model displayed consistent performance across various evaluation metrics, including the R2 score, MSE, and RMSE. Its high performance on both the training and test data indicates good capabilities and reduces the risk of overfitting.\n",
        "\n",
        "Additionally the residuals analysis revealed mean and median values of 0 and -.09, respectively.The well-behaved residuals indicate that the XGBoost model effectively captures the underlying patterns in the data and minimizes systematic errors.\n",
        "\n",
        "Considering the model's strong predictive performance, lower prediction errors, and consistency across evaluation metrics, the XGBoost model is the preferred choice as the final prediction model.\n"
      ],
      "metadata": {
        "id": "-ct0HcUlprH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}